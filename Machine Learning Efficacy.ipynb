{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025d3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3dc34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0fa5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def evaluate_classifiers(synthetic_dataset, real_dataset, target_column):\n",
    "    # Check if the dataset contains categorical variables\n",
    "    if synthetic_dataset.select_dtypes(include=['object']).shape[1] > 0:\n",
    "        # Concatenate the synthetic and real datasets\n",
    "        combined_data = pd.concat([synthetic_dataset, real_dataset], ignore_index=True)\n",
    "\n",
    "        # Apply one-hot encoding to handle categorical variables\n",
    "        encoder = OneHotEncoder()\n",
    "        combined_data_encoded = encoder.fit_transform(combined_data)\n",
    "\n",
    "        # Separate the synthetic and real datasets\n",
    "        new_data2_encoded = combined_data_encoded[:len(synthetic_dataset)]\n",
    "        real_data_encoded = combined_data_encoded[len(synthetic_dataset):]\n",
    "\n",
    "        synthetic_X = new_data2_encoded[:, :-1].toarray()\n",
    "        synthetic_y = new_data2_encoded[:, -1].toarray()\n",
    "        real_train_X, real_test_X, real_train_y, real_test_y = train_test_split(\n",
    "            real_data_encoded[:, :-1].toarray(), real_data_encoded[:, -1].toarray(), test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        synthetic_X = synthetic_dataset.drop(columns=[target_column]).to_numpy()\n",
    "        synthetic_y = synthetic_dataset[target_column].to_numpy()\n",
    "        real_train_X, real_test_X, real_train_y, real_test_y = train_test_split(\n",
    "            real_dataset.drop(columns=[target_column]).to_numpy(), real_dataset[target_column].to_numpy(),\n",
    "            test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "    # Train decision tree classifier on real_train dataset\n",
    "    decision_tree_real = DecisionTreeClassifier()\n",
    "    decision_tree_real.fit(real_train_X, real_train_y)\n",
    "    decision_tree_pred_real = decision_tree_real.predict(real_test_X)\n",
    "    f1_score_decision_tree_real = f1_score(real_test_y, decision_tree_pred_real)\n",
    "\n",
    "    # Train decision tree classifier on synthetic dataset\n",
    "    decision_tree_synthetic = DecisionTreeClassifier()\n",
    "    decision_tree_synthetic.fit(synthetic_X, synthetic_y)\n",
    "    decision_tree_pred_synthetic = decision_tree_synthetic.predict(real_test_X)\n",
    "    f1_score_decision_tree_synthetic = f1_score(real_test_y, decision_tree_pred_synthetic)\n",
    "\n",
    "    # Train gradient boost classifier on real_train dataset\n",
    "    gradient_boost_real = GradientBoostingClassifier()\n",
    "    gradient_boost_real.fit(real_train_X, real_train_y)\n",
    "    gradient_boost_pred_real = gradient_boost_real.predict(real_test_X)\n",
    "    f1_score_gradient_boost_real = f1_score(real_test_y, gradient_boost_pred_real)\n",
    "\n",
    "    # Train gradient boost classifier on synthetic dataset\n",
    "    gradient_boost_synthetic = GradientBoostingClassifier()\n",
    "    gradient_boost_synthetic.fit(synthetic_X, synthetic_y)\n",
    "    gradient_boost_pred_synthetic = gradient_boost_synthetic.predict(real_test_X)\n",
    "    f1_score_gradient_boost_synthetic = f1_score(real_test_y, gradient_boost_pred_synthetic)\n",
    "\n",
    "    # Create a dictionary to store the F1-scores\n",
    "    f1_scores = {\n",
    "        'Decision Tree (Real)': f1_score_decision_tree_real,\n",
    "        'Decision Tree (Synthetic)': f1_score_decision_tree_synthetic,\n",
    "        'Gradient Boosting (Real)': f1_score_gradient_boost_real,\n",
    "        'Gradient Boosting (Synthetic)': f1_score_gradient_boost_synthetic,\n",
    "    }\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your synthetic dataset is stored in the variable 'new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fecb9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data=pd.read_csv('german_credit_data.csv')\n",
    "new_data2=pd.read_csv('new_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns to be one-hot encoded\n",
    "categorical_columns = ['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Purpose', 'Risk']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_constant, columns=categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bfc076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
